# Web scrapping: Golden Pages

## Working out errors related to the start of the code
If an error occurs when running the code after installing the necessary libraries, then most likely the Chrome web driver for your OS is not installed. How to install the web driver can be found at this link. [selenium-python.com](https://selenium-python.com/install-chromedriver-chrome?ysclid=ly2ufnhjip111135754)
If the code does not work after installing the web driver or an error occurs, then you need to check whether the Chrome browser is open in memory. If so, you need to unload it (close it). This is due to the fact that the Chrome browser launched by the code runs in the background in the PC memory and may conflict with the Chrome browser running at the same time.
If the error still occurs, then you need to check or reinstall the libraries you are using.

## Working with the parser
- To start parsing the site [goldenpages.uz](https://www.goldenpages.uz/search/) it is required to create an array of site search queries, the variable task_array = ['кафе в самарканде', '', 'кафе в намангане', 'кафе в бухаре']. GoldenPages site restriction - queries must contain at least three characters, otherwise the output of the site search result will be empty.
- To start parsing through an array of queries and save the resulting frame of paired data, you need to call the command parsed_data = GoldenPagesParser(task_array = task_array).task_frame_parser() which will return the Pandas Data Frame with the result of parsing to the parsed_data variable.
- To start parsing the site https://www.goldenpages.uz/search/ you need to create an array of site search queries, the variable url_task_array  = ['https://www.goldenpages.uz/search/?region=&s=кафе%20в%20самарканде  ', ", 'https://www.goldenpages.uz/search/?region==кафе%20в%20самарканде ', 'https://www.goldenpages.uz/rubrics /?Id=3480', 'https://www.goldenpages.uz/rubrics/?Id=105284 ', 'https://www.goldenpages.uz/search/?region=&s=кафе  ', 'https://www.goldenpages.uz/rubrics /?Id=3052']. Restriction - requests must contain only the URLs of the first pages of the search results of interest, the iterator will collect the rest independently, if the link contains the result in the form of links of the found groups, the iterator will also work out such a request independently by collecting all the data of all pages that the iterator finds for these links, they do not need to be additionally included in the request.
- To start parsing through an array of URL requests and save the resulting frame of the paired data, you need to call the command parsed_data = GoldenPagesParser(url_task_array = url_task_array).task_frame_parser() which will return the Pandas Data Frame with the result of parsing to the parsed_data variable.
- The code will contain the following columns of collected data: id type 'int' - the number of the collected cell of each request, company_name, company_second_name, company_third_name, address, landmarks, phone_array, site_url, parsing_url - a link to the page from which the data was collected, parsing_task - the query in which the data was collected.
- To save the resulting frame to a CSV file, you need to uncomment the line of code parsed_data.to_csv(f"Selenium_Soup_GoldenPages_{datetime.now().strftime('%d%m%Y')}.csv"), which saves the data to a file named "Selenium_Soup_GoldenPages_01072024.csv" and the folder with the code being run. The date will be inserted automatically - the current date of saving the parsing result.

## Additionally
This script is provided for educational and informational purposes only.
